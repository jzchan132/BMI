# Exploring Input Atributions

With the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important.

In these notebooks, we demonstrate how to generate model explanations with integrated gradients from the Captum library. We then explore how different layer attribution algorithms from Captum interact with various datasets to produce useful heuristics.
